# Advanced Quantization Techniques for Efficient AI Inference

## Introduction  
Modern AI inference demands **maximum efficiency** in compute and memory. Quantization – representing neural network parameters and activations with reduced precision – is a key tool to shrink model size and speed up inference. While 8-bit integer or floating-point formats are now common, the state-of-the-art is rapidly moving toward **ultra-low precisions (4-bit and below)** to further improve efficiency. For example, Nvidia’s Blackwell GPU introduces an FP4 format (MXFP4, 2 exponent bits, 1 mantissa bit) with shared exponents (UE4M3) to push beyond FP8 ([CUDA - Wikipedia](https://en.wikipedia.org/wiki/CUDA))  To truly exploit such low precisions, researchers have developed new quantization methods that minimize accuracy loss even at 4-bit, 2-bit, or even 1-bit per weight. This report surveys these **cutting-edge quantization techniques** with an emphasis on hardware-optimal approaches. We focus on proven methods that enable **maximal inference throughput** with minimal accuracy drop, and we highlight which techniques clearly dominate others. Methods that are outdated or inferior are omitted in favor of the latest advances. We also touch on related strategies like structured sparsity and emerging analog computing paradigms, which complement quantization in the quest for efficiency. The aim is to provide a comprehensive yet focused guide to the **most advanced quantization strategies** available for AI hardware designers and practitioners.

## Ultra-Low Precision Quantization Advances  
Conventional post-training quantization often struggles below 8-bit precision, but new methods have pushed into the 4-bit and sub-4-bit regime with excellent results. Many of these innovations combine clever mathematical techniques (like transforms or codebooks) with insights into neural network weight distributions. Below, we outline several state-of-the-art quantization approaches that enable extremely low bit-widths without retraining, and note when one approach supersedes another.

### Hadamard Transforms for Incoherent Quantization (QuIP and QuIP#)  
One breakthrough in low-bit post-training quantization is the use of **orthogonal transforms (Hadamard rotations)** to decorrelate weights before quantization. The QuIP method (Chee et al. 2023) introduced an *incoherence processing* step that multiplies weight matrices by a random orthogonal (e.g. Hadamard) matrix so that the transformed weights behave like i.i.d. Gaussian noise ([Even Better, Even Faster Quantized LLMs with QTIP](https://together.ai/blog/even-better-even-faster-quantized-llms-with-qtip))  This makes quantization easier by removing outlier directions. QuIP then applies optimized rounding to minimize quantization error. This approach made **2-bit weight quantization viable for the first time on large language models** ([GitHub - pprp/Awesome-LLM-Quantization: Awesome list for LLM quantization](https://github.com/pprp/Awesome-LLM-Quantization))  a regime where prior methods failed. 

Building on this, **QuIP#** (Tseng et al. 2024) further refines the idea with a randomized Hadamard transform (fast and with strong theoretical guarantees) ([[2402.04396] QuIP#: Even Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks](https://arxiv.org/abs/2402.04396))  QuIP# is a weight-only post-training quantization (PTQ) method that achieves **state-of-the-art accuracy at ≤4 bits per weight**. In addition to the transform, QuIP# introduces other innovations: (1) it incorporates **vector quantization codebooks** based on the *E8 lattice* to quantize groups of weights with minimal distortion, and (2) it optionally performs a light fine-tuning on the quantized model to recover any remaining accuracy loss ([[2402.04396] QuIP#: Even Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks](https://arxiv.org/abs/2402.04396))  These techniques combined allow QuIP# to significantly outperform earlier PTQ methods like GPTQ in the ultra-low precision range ([[2402.04396] QuIP#: Even Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks](https://arxiv.org/abs/2402.04396))  In fact, QuIP# has been shown to dominate prior methods on 3-bit and 2-bit weight compression, often matching FP16 baseline accuracy on large models ([[2402.04396] QuIP#: Even Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks](https://arxiv.org/abs/2402.04396))  By leveraging Hadamard-based weight “incoherence” and lattice-optimized quantization, QuIP# represents the cutting edge in extreme weight compression.

*Key features and results of QuIP#:*  
- Uses random **Hadamard transforms** to decorrelate weight matrices, yielding nearly Gaussian weight distributions ([Even Better, Even Faster Quantized LLMs with QTIP](https://together.ai/blog/even-better-even-faster-quantized-llms-with-qtip))  This reduces worst-case quantization error.  
- Quantizes weights using **vector codebooks** derived from the optimal `E_8` lattice packing in 8 dimensions for minimal distortion ([[2402.04396] QuIP#: Even Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks](https://arxiv.org/abs/2402.04396))  This hardware-efficient VQ achieves better accuracy than per-weight quantization.  
- Achieves **state-of-the-art performance at 2–4 bits** per weight, outperforming earlier PTQ methods ([[2402.04396] QuIP#: Even Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks](https://arxiv.org/abs/2402.04396))  QuIP made 2-bit LLM quantization achievable ([GitHub - pprp/Awesome-LLM-Quantization: Awesome list for LLM quantization](https://github.com/pprp/Awesome-LLM-Quantization))  and QuIP# improves quality further.  
- Requires no or minimal retraining. A short fine-tuning step can be applied to further improve fidelity ([[2402.04396] QuIP#: Even Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks](https://arxiv.org/abs/2402.04396))  though the core method is post-training.

**Why it dominates**: QuIP#’s combination of incoherence processing and lattice-based quantization yields lower error at 2–4 bits than conventional uniform or even prior adaptive methods. It effectively eliminates the heavy outliers that make naive low-bit quantization fail, allowing unprecedented compression with minimal accuracy loss. Thus, methods like GPTQ or simple KMeans codebooks are generally inferior in the ≤4-bit regime when QuIP# is applicable.

### Lattice Codebooks and Vector Quantization (Additive Quantization)  
An alternative family of techniques uses **vector quantization (VQ)** or learned codebooks to compress weights into lower precision. Instead of quantizing each weight independently, these methods represent groups of weights by indices into a codebook (or a sum of multiple codebook vectors), capturing more information per bit. Traditional vector quantization was limited by high complexity as group size grows, but recent advances have made it practical and extremely effective for neural networks.

One example already mentioned is QuIP#’s use of an `E_8` lattice codebook: by quantizing weight vectors in 8-dimensional blocks that tile the weight matrix, it achieves near-optimal packing of values inside an 8-D unit sphere ([[2402.04396] QuIP#: Even Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks](https://arxiv.org/abs/2402.04396))  The codebook is designed for hardware efficiency (the `E_8` lattice has a very regular structure), avoiding a large memory overhead. This approach showed better accuracy than scalar quantization at the same bit-rate.

Going further, **additive quantization** techniques represent a weight (or a small group of weights) as a *sum of multiple codebook vectors*. A recent method called **AQLM (Additive Quantization for LLM)** employs *multi-codebook quantization* to achieve extremely low effective bit-widths ([GitHub - pprp/Awesome-LLM-Quantization: Awesome list for LLM quantization](https://github.com/pprp/Awesome-LLM-Quantization))  In AQLM, several learned codebooks are trained such that each weight vector can be composed as (for example) the sum of two or three codebook entries. By tuning these codebooks (with a small calibration or training), AQLM can compress large model weights to an effective 2–3 bits per weight with very high accuracy ([GitHub - pprp/Awesome-LLM-Quantization: Awesome list for LLM quantization](https://github.com/pprp/Awesome-LLM-Quantization))  Crucially, AQLM’s joint optimization across entire layers yields **Pareto-optimal results in the sub-3-bit range**, significantly outperforming prior methods at 2 bits ([GitHub - pprp/Awesome-LLM-Quantization: Awesome list for LLM quantization](https://github.com/pprp/Awesome-LLM-Quantization))  In other words, for a given accuracy loss, AQLM achieves the lowest bit-width (or conversely, for a given memory budget, it achieves the highest accuracy), making it a dominating approach when aggressive compression is needed. Efficient implementations also demonstrate practical speedups over FP16 inference despite the indirection of codebook lookups ([GitHub - pprp/Awesome-LLM-Quantization: Awesome list for LLM quantization](https://github.com/pprp/Awesome-LLM-Quantization)) 

Other codebook-based methods include product quantization and clustering approaches used historically for model compression. However, many older codebook methods (e.g. simple `k`-means clustering of weights into 256 centers for 8-bit storage) have been superseded by these new lattice or additive quantization techniques. The modern methods train or design codebooks in a way that is much more compatible with hardware and preserves accuracy at very low precision. They often operate on small blocks of weights (to localize approximation error) and use structures that can be efficiently decoded.

*Key points on vector codebook quantization:*  
- **Block-wise vector quantization** uses one code per group of weights instead of each weight individually. This captures correlations and often reduces error. Recent innovations use structured codebooks (like lattices) to keep lookup efficient ([[2402.04396] QuIP#: Even Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks](https://arxiv.org/abs/2402.04396))   
- **Additive (multi-codebook) quantization** represents weights as a sum of a few basis vectors. This dramatically expands the representable set of values with only a slight increase in codebook storage. AQLM is a leading example, achieving **2–3 bit effective precision with minimal loss**, and outperforming prior 2-bit methods ([GitHub - pprp/Awesome-LLM-Quantization: Awesome list for LLM quantization](https://github.com/pprp/Awesome-LLM-Quantization))   
- Codebook methods shine in the ultra-low precision regime, where uniform quantization error becomes intolerable. They have been **proven to yield better accuracy–compression trade-offs** than scalar quantizers at ≤4 bits per weight ([GitHub - pprp/Awesome-LLM-Quantization: Awesome list for LLM quantization](https://github.com/pprp/Awesome-LLM-Quantization))   
- Potential downside is decoding complexity, but many methods constrain codebooks for fast decoding. For instance, using a small fixed-group size or power-of-two-weighted code vectors can allow efficient implementation. The success of AQLM and QuIP# demonstrates that these approaches can be made practical on GPUs/CPUs today ([GitHub - pprp/Awesome-LLM-Quantization: Awesome list for LLM quantization](https://github.com/pprp/Awesome-LLM-Quantization)) 

In summary, **vector and lattice quantization techniques currently dominate uniform quantization** in the extreme low-bit regime. They are “overlooked” no more – methods like AQLM have established new state-of-the-art baselines for 2–4 bit compression. Simpler codebook schemes that lack these innovations are now considered inferior.

### Trellis-Coded Quantization (QTIP)  
While vector quantization uses codebooks for groups of weights, another cutting-edge approach is to use **trellis-coded quantization (TCQ)** to encode weight blocks. **QTIP** (Quantization with Trellises and Incoherence Processing) is a 2024 method that has achieved a new state-of-the-art combination of accuracy and speed for LLM weight compression ([Even Better, Even Faster Quantized LLMs with QTIP](https://together.ai/blog/even-better-even-faster-quantized-llms-with-qtip))  It builds on the QuIP# framework (using the same Hadamard-based incoherence processing to Gaussianize weight distributions) but replaces the lattice vector quantization with trellis coding ([Even Better, Even Faster Quantized LLMs with QTIP](https://together.ai/blog/even-better-even-faster-quantized-llms-with-qtip)) 

**Trellis-Coded Quantization** comes from communications theory – it’s essentially a form of vector quantization that uses *sequence coding* (with a trellis diagram) to encode a long string of values efficiently. The advantage of TCQ is that its complexity scales linearly with the dimensionality of the data, rather than exponentially as in brute-force vector quantization ([Even Better, Even Faster Quantized LLMs with QTIP](https://together.ai/blog/even-better-even-faster-quantized-llms-with-qtip))  This means QTIP can quantize very large groups or even entire weight matrices as one “sequence,” achieving much lower distortion than small-block VQ methods ([Even Better, Even Faster Quantized LLMs with QTIP](https://together.ai/blog/even-better-even-faster-quantized-llms-with-qtip))  On i.i.d. Gaussian data (which the Hadamard transform makes the weights resemble), TCQ is near-optimal in rate–distortion sense. The authors report that QTIP **significantly improves on QuIP#’s accuracy** (which was already best-in-class) while still being *&gt;3× faster in inference throughput than an unquantized model* ([Even Better, Even Faster Quantized LLMs with QTIP](https://together.ai/blog/even-better-even-faster-quantized-llms-with-qtip)) 

A challenge with applying TCQ to neural networks is ensuring the quantized weights can be *decoded quickly* during inference. Naively, a TCQ encoding could introduce non-trivial overhead or require serial decoding that slows things down. QTIP solves this by designing a specialized “bitshift trellis” and **compute-friendly codes** that trade off a bit of extra computation to avoid memory overhead ([Even Better, Even Faster Quantized LLMs with QTIP](https://together.ai/blog/even-better-even-faster-quantized-llms-with-qtip))  In effect, they ensure the quantized weights can be reconstructed on-the-fly with minimal latency, allowing the model to run quickly. The result is a weight-only quantization scheme that offers **record-low quantization error** at extremely low precisions, without sacrificing speed or requiring retraining.

*Why QTIP is notable:*  
- It leverages **TCQ to encode longer weight sequences** than prior VQ methods, yielding lower distortion for the same bit budget ([Even Better, Even Faster Quantized LLMs with QTIP](https://together.ai/blog/even-better-even-faster-quantized-llms-with-qtip))  This pushes quantization closer to information-theoretic limits for Gaussian weight distributions.  
- Through careful engineering (bitshift trellis, etc.), it maintains **hardware-friendly decoding**, avoiding the exponential complexity of high-dimensional vector codebooks ([Even Better, Even Faster Quantized LLMs with QTIP](https://together.ai/blog/even-better-even-faster-quantized-llms-with-qtip))  ([Even Better, Even Faster Quantized LLMs with QTIP](https://together.ai/blog/even-better-even-faster-quantized-llms-with-qtip))   
- Empirically, QTIP has **surpassed QuIP# in accuracy** on 2–3 bit weight quantization while preserving high throughput ([Even Better, Even Faster Quantized LLMs with QTIP](https://together.ai/blog/even-better-even-faster-quantized-llms-with-qtip))  This dominance suggests that trellis-coded approaches can outperform lattice codebook methods when properly optimized.  
- QTIP exemplifies a new direction that treats quantization as encoding problem (like data compression) – a shift from the traditional per-weight rounding mindset. This could inspire further techniques that bring ideas from source coding theory into quantization of neural nets.

In summary, QTIP currently represents the frontier of weight-only post-training quantization: it **dominates earlier methods like QuIP#** in the ultra-low precision range, and thus earlier inferior methods (including older GPTQ-style rounding or simple clustering) are effectively obsoleted for those use cases.

### Mixed Precision and Selective Bit Allocation  
Another crucial strategy for pushing quantization to its limits is **mixed-precision quantization** – allocating different bit-widths to different parts of the network based on their sensitivity. Not all weights and activations have equal impact on model accuracy; some can be more aggressively quantized than others. State-of-the-art techniques exploit this by identifying “salient” weights or channels that require higher precision and quantizing the rest more heavily. The result is a highly compressed model that retains accuracy by judiciously spending bits where they matter most.

One recent example is **SliM-LLM (Salience-Driven Mixed-Precision for LLMs)**. In this approach, a large model (targeting ~2-bit average weight precision) is analyzed to determine which groups of weights are most important (salient) to the model’s output. Using a criterion like Kullback-Leibler divergence or output sensitivity, it then assigns a higher bit-width (e.g. 3-4 bits) to those important groups and lower (e.g. 2 bits or even 1) to less critical groups ([GitHub - pprp/Awesome-LLM-Quantization: Awesome list for LLM quantization](https://github.com/pprp/Awesome-LLM-Quantization))  This *Salience-Determined Bit Allocation* is solved as an optimization problem to best approximate the original model’s outputs under a total bit budget ([GitHub - pprp/Awesome-LLM-Quantization: Awesome list for LLM quantization](https://github.com/pprp/Awesome-LLM-Quantization))  Additionally, SliM-LLM applies a *salience-weighted quantizer calibration*, essentially fine-tuning the quantization parameters with greater emphasis on the important weights. The end result is a compressed model that uses a **mix of precisions across layers, achieving better accuracy than any uniform-bit model of the same size**. Techniques like this **dominate naive uniform quantization**, especially at extremely low precisions, because they avoid catastrophic error concentration in sensitive parts of the network.

Along similar lines, there have been attempts at **1-bit and 2-bit mixed schemes**. For example, **PB-LLM (Partially Binarized LLM)** and **BiLLM (Binary LLM)** split each weight matrix into a small subset of weights kept at higher precision and the majority binarized ([GitHub - pprp/Awesome-LLM-Quantization: Awesome list for LLM quantization](https://github.com/pprp/Awesome-LLM-Quantization))  ([GitHub - pprp/Awesome-LLM-Quantization: Awesome list for LLM quantization](https://github.com/pprp/Awesome-LLM-Quantization))  PB-LLM identifies a small fraction of “exception” weights that are too important to binarize and leaves them at say 8-bit, while binarizing the rest; BiLLM uses a binary + residual approach, splitting weights into two groups and providing a bit of correction for the binarized group ([GitHub - pprp/Awesome-LLM-Quantization: Awesome list for LLM quantization](https://github.com/pprp/Awesome-LLM-Quantization))  Impressively, BiLLM (a PTQ method) achieved perplexity 8.41 on LLaMA2-70B using effectively ~1.08 bits per weight ([GitHub - pprp/Awesome-LLM-Quantization: Awesome list for LLM quantization](https://github.com/pprp/Awesome-LLM-Quantization)) – a notable result showing that even **1-bit average precision is not entirely out of reach** with clever grouping. However, the accuracy is still significantly lower than higher-bit methods (8.41 ppl vs ~5 for full precision), so such extreme binarization is more of a specialized achievement than a broadly dominating technique. Nonetheless, these works underscore the value of **heterogeneous precision**: by *omitting inferior uniform approaches*, mixed-precision methods squeeze more performance out of a given bit budget.

It’s worth noting that choosing mixed precision can be done via analytical criteria (e.g. Hessian-based sensitivity as in some older works) or via small-scale brute force search/calibration. Some frameworks (like OmniQuant, 2024) even treat bit-allocation as learnable parameters and optimize them with a bit of gradient descent on a calibration set ([GitHub - pprp/Awesome-LLM-Quantization: Awesome list for LLM quantization](https://github.com/pprp/Awesome-LLM-Quantization))  This blurs the line between PTQ and QAT – OmniQuant trains *only the quantization parameters* (like clipping ranges per layer) while keeping weights fixed, reaching near-QAT quality with much less effort ([GitHub - pprp/Awesome-LLM-Quantization: Awesome list for LLM quantization](https://github.com/pprp/Awesome-LLM-Quantization))  The common theme is focusing effort (be it additional bits or a bit of training) on the most error-prone parts of the network, rather than uniformly quantizing everything and having the worst layer dictate the overall accuracy.

*In summary:* New mixed-precision quantization techniques **outperform single-precision schemes** by tailoring the precision to each layer or group’s needs. They represent a fundamental shift from prior “one-size-fits-all” quantization, and any inferior method that neglects per-layer variation is now considered suboptimal. For hardware, mixed precision can be slightly more complex to implement (since the accelerator must handle multiple precisions), but many AI chips today do support mixed precision or at least per-layer precision switching. The payoff in accuracy for a given model size is well worth it – enabling extreme compression without the collapse in quality that a uniform low-bit model would suffer.

### Quantization-Aware Training and Distillation Enhancements  
While the focus of this survey is on post-training methods (since they are most practical for retrofitting models on hardware), it’s important to mention that the absolute highest accuracy at low precision often comes from some form of **quantization-aware training (QAT)**. “Light” fine-tuning has already been mentioned (e.g. QuIP#’s optional finetune, OmniQuant’s calibration of parameters). Pushing that further, **BitDistiller** (Du et al. 2024) is a framework that uses *self-distillation* during QAT to boost sub-4-bit performance ([GitHub - pprp/Awesome-LLM-Quantization: Awesome list for LLM quantization](https://github.com/pprp/Awesome-LLM-Quantization))  It trains a quantized model (e.g. 3-bit weights) using knowledge distillation from the full-precision model’s logits, along with tailored loss functions that emphasize preserving important predictions ([GitHub - pprp/Awesome-LLM-Quantization: Awesome list for LLM quantization](https://github.com/pprp/Awesome-LLM-Quantization))  By combining distillation with techniques like asymmetric quantizers and learned clipping, BitDistiller achieved strong results at 2–3 bits ([GitHub - pprp/Awesome-LLM-Quantization: Awesome list for LLM quantization](https://github.com/pprp/Awesome-LLM-Quantization))  In fully training scenarios, researchers have even demonstrated training vision models from scratch with 4-bit weights/activations (using sophisticated optimizers and quantization functions), indicating that future networks might be trained inherently to tolerate ultra-low precision.

For inference deployment, pure QAT (retraining the entire model for quantization) is often not feasible due to time and data constraints, especially with very large models. However, these techniques serve as a **proof of what’s achievable**. They also often produce insights (like which layers are hardest to quantize, or which quantizer design works best) that feed back into improved PTQ methods. For instance, if QAT finds that a certain activation layer needs 5 bits to not bottleneck accuracy, a mixed-precision PTQ scheme can assign that layer 5 bits while others use 4 or 3. Therefore, while we generally **stick to proven PTQ techniques** for immediate deployment, QAT-enhanced results set the upper bound and have largely confirmed that the advanced PTQ approaches above are hitting very close to that bound. In short, minimal-gap exists between the best PTQ (e.g. QTIP or AQLM) and QAT when these modern strategies are employed ([GitHub - pprp/Awesome-LLM-Quantization: Awesome list for LLM quantization](https://github.com/pprp/Awesome-LLM-Quantization))  ([GitHub - pprp/Awesome-LLM-Quantization: Awesome list for LLM quantization](https://github.com/pprp/Awesome-LLM-Quantization))  which is a testament to how far quantization has progressed.

## Hardware-Efficient Numerical Formats (FP8, FP4, etc.)  
Achieving efficient low-bit inference isn’t just about algorithms; it also relies on **numeric formats** that are friendly to hardware and model accuracy. Traditional quantization used 8-bit integers with a shared scale (zero-point), or 16-bit floats (e.g. IEEE FP16). In recent years, we’ve seen a proliferation of new number formats specifically designed for AI – especially 8-bit and 4-bit floating-point variants and hybrid schemes. The goal of these formats is to maximize the usable dynamic range and precision for neural network values while minimizing bit-width. Here we survey the key formats and their advantages:

### FP8 and FP4 Microscaling Formats
**8-bit floating point (FP8)** has rapidly become a de facto standard in cutting-edge AI hardware. Nvidia’s H100 GPU introduced support for FP8 in two flavors, E4M3 and E5M2 (4 exponent bits, 3 mantissa bits; or 5 exponent, 2 mantissa) ([CUDA - Wikipedia](https://en.wikipedia.org/wiki/CUDA))  These formats allow a dynamic range much wider than INT8 (thanks to the exponent) while still using only 8 bits. For example, E5M2 covers a range roughly equal to FP16’s range but with less precision ([CUDA - Wikipedia](https://en.wikipedia.org/wiki/CUDA))  This flexibility lets networks with heavy-tailed weight or activation distributions quantize to 8-bit without clipping outliers as aggressively. Indeed, research found FP8 quantization generally preserves accuracy better than INT8 on tough networks, and training techniques have adapted to FP8 for both weights and gradients. FP8 has been **proven in production** for both training and inference (Nvidia, Intel Habana, and others have published results on large models using FP8 with negligible accuracy loss). It’s now a “default choice in many hardware platforms” for low-precision arithmetic ([LLM-FP4: 4-Bit Floating-Point Quantized Transformers - ACL Anthology](https://aclanthology.org/2023.emnlp-main.39/))

Going even further, the industry is coalescing around **4-bit floating point** formats for inference. The **MXFP4 (E2M1) format** (part of the OCP Microscaling standard) uses a 4-bit value consisting of 1 sign bit, 2 exponent bits, and 1 mantissa bit, coupled with a shared scaling factor ([CUDA - Wikipedia](https://en.wikipedia.org/wiki/CUDA))  ([cuBLAS](https://docs.nvidia.com/cuda/pdf/CUBLAS_Library.pdf))  In practice, this is implemented as **block floating-point**: e.g., every block of 16 values shares an 8-bit scale (exponent/mantissa) while each value has its own 4-bit (signed) significand ([cuBLAS](https://docs.nvidia.com/cuda/pdf/CUBLAS_Library.pdf))  Nvidia’s Blackwell-generation GPUs are expected to support this FP4 with a per-16 scaling (the “ue4m3” 7-bit scale format) ([cuBLAS](https://docs.nvidia.com/cuda/pdf/CUBLAS_Library.pdf))  The advantage of FP4 + shared scale is that it provides a *little* precision per value but a lot of dynamic range per block. This tackles the main difficulty of 4-bit quantization, which is that a fixed scaling factor for an entire tensor (as in INT4) often cannot represent both large and small values simultaneously. By microscaling each block, FP4 avoids outlier distortion – each block gets its exponent adjusted optimally.

Recent research backs the effectiveness of these FP4 schemes. In **LLM-FP4 (Liu et al. 2023)**, the authors quantized both weights and activations of a 13B LLM to 4-bit floating point and achieved only a small drop in performance (~5.8 points on a 0-100 scale) – significantly better than prior INT4 approaches ([LLM-FP4: 4-Bit Floating-Point Quantized Transformers - ACL Anthology](https://aclanthology.org/2023.emnlp-main.39/))  They note that *floating-point quantization is more flexible than integer for low bits*, better handling long-tail distributions ([LLM-FP4: 4-Bit Floating-Point Quantized Transformers - ACL Anthology](https://aclanthology.org/2023.emnlp-main.39/))  A key insight was choosing the right number of exponent bits: FP4’s success depends on using enough exponent range. The E2M1 format (2 exponent, 1 mantissa) was found to outperform pure INT4 because it can represent values across a wider range (though with coarse steps) ([LLM-FP4: 4-Bit Floating-Point Quantized Transformers - ACL Anthology](https://aclanthology.org/2023.emnlp-main.39/))  Another trick was **per-channel scaling for activations**, effectively a block FP scheme for activation tensors, which they showed could be folded into weights with negligible overhead ([LLM-FP4: 4-Bit Floating-Point Quantized Transformers - ACL Anthology](https://aclanthology.org/2023.emnlp-main.39/))  All told, FP4 with microscaling matched or exceeded the accuracy of INT8 in some cases, demonstrating dominance over older 4-bit integer quantization that lacked dynamic range ([LLM-FP4: 4-Bit Floating-Point Quantized Transformers - ACL Anthology](https://aclanthology.org/2023.emnlp-main.39/))  ([LLM-FP4: 4-Bit Floating-Point Quantized Transformers - ACL Anthology](https://aclanthology.org/2023.emnlp-main.39/))

*In summary, FP8 and FP4 formats:*
- **FP8 (E4M3, E5M2)** – 8-bit floats now broadly supported (Nvidia, Intel, etc.). They retain much more accuracy on tough networks than INT8 due to exponent flexibility. FP8 has essentially replaced INT8 in many cutting-edge deployments for both inference and even training, with minimal downsides aside from slightly more complex hardware (which has been implemented).
- **FP4 (E2M1 with block scaling)** – A 4-bit float used in tandem with a shared 8-bit scale per block (e.g. per 16 values) ([cuBLAS](https://docs.nvidia.com/cuda/pdf/CUBLAS_Library.pdf))  This **microscaling** approach drastically improves accuracy vs naive INT4, and is being standardized (OCP MXFP4) and implemented in hardware. It dominates simpler 4-bit formats by providing needed dynamic range. Early results show networks can run at 4-bit weights/acts with tolerable accuracy loss when using block FP4 ([LLM-FP4: 4-Bit Floating-Point Quantized Transformers - ACL Anthology](https://aclanthology.org/2023.emnlp-main.39/))
- These formats are optimized for hardware: the shared scaling factors add a small memory overhead (e.g. 1 scale per 16 values = +6.25% overhead), but hardware can be designed to apply the scale on the fly with minimal extra cycles. The Open Compute Project’s standardization indicates multiple vendors see this as the path forward ([AMD, Arm, Intel, Meta, Microsoft, NVIDIA, and Qualcomm ...](https://opencompute.org/blog/amd-arm-intel-meta-microsoft-nvidia-and-qualcomm-standardize-next-generation-narrow-precision-data-formats-for-ai))
- **Dominance**: FP8 has largely made alternative 8-bit schemes (like int8 with per-tensor scale or fixed-point 8-bit) obsolete in high-end applications – it’s simply more accurate for the same bit count. Likewise, FP4 with block scaling is expected to make plain int4 (with a single scale) an inferior choice except perhaps in the most latency-sensitive edge cases.

### Block Floating Point and Logarithmic Quantization
The FP4 example above is a special case of a more general concept: **Block Floating Point (BFP)**. BFP means a group of numbers share an exponent, essentially quantizing numbers with a common scale. This idea has been around – for instance, Intel’s now-defunct Nervana NNP processors used a form of block floating (Flexpoint) for training, and some DSPs handle vectors in block floating formats. What’s new is the **granularity and usage**: modern microscaling uses small blocks (16 or 32) and is applied to inference weights/acts to minimize error. Block FP is very hardware-friendly because within each block you can treat mantissas as fixed-point and use a common exponent adjustment. The hardware cost is a bit of logic to align exponents and the storage for those exponents. As noted, the industry is converging on standard block sizes (16 or 32) for FP4/FP8 to balance flexibility and overhead ([cuBLAS](https://docs.nvidia.com/cuda/pdf/CUBLAS_Library.pdf))  Block floating formats **dominate purely fixed-point formats** at low precision, since they reduce the quantization error dramatically for values that have any significant variance.

Another “overlooked” quantization strategy with hardware merit is **logarithmic quantization**. Instead of linear steps, log quantization uses values distributed geometrically (often base-2). A simple version is to constrain quantized weights to powers of two (or sign and power of two), so that multiplication can be replaced by bit shifting. This was explored in some works (sometimes called “Power-of-Two quantization” or Log-Q). It tends to hurt accuracy unless many bits are used, but one interesting method (APoT: Additive Powers-of-Two, 2019) represented weights as a sum of a few power-of-two terms, combining the ideas of codebooks and log-scale quantization. APoT achieved good accuracy with 2–4 bit equivalents by using multiple power-of-two terms per weight. The benefit is that multiply-accumulates turn into a few shift-accumulates, which are extremely hardware efficient.

However, in terms of **proven advantage**, log-based quantization hasn’t seen wide adoption compared to standard linear or floating quantization. It remains a niche that could be useful when multipliers are very expensive (e.g. tiny microcontrollers or certain FPGA designs). In most AI accelerators, multipliers are abundant and fast, so the main goal is to reduce memory and data movement – which any low-bit scheme does, regardless of linear vs log scale. Thus, while one might consider logarithmic quantization as an alternative, it’s generally dominated by the more flexible floating-point and integer methods which allow fine tuning of scales and biases.

Finally, a mention of **posit number systems**: Posits are an alternative to IEEE floats proposed by Gustafson, offering dynamic precision and regime bits for exponent scaling. In theory, posits could be more accurate per bit than IEEE floats for certain value distributions. There has been interest in using posits for neural networks to squeeze more precision out of fewer bits (e.g. a posit8 might outperform an FP8). Some prototype implementations and studies showed posits can work for inference, but **no major hardware has adopted posits** to date. The complication of variable-length encoding and lack of standardization make them less appealing. Given the momentum behind standard FP8/FP4, posits remain speculative in the AI context. Similarly, any exotic numeric format not backed by hardware is beyond our scope – we focus on what’s practical. In summary, block floating-point (as in MX formats) is the *proven winner* for aggressive quantization, whereas purely logarithmic or posit approaches have not demonstrated a clear enough benefit to overtake the emerging standards.

## Structured Sparsity for Further Speedup
Quantization is not the only way to trim the fat in neural networks. **Pruning and sparsity** – removing weights or activations that are zero – can also reduce computation. However, to be hardware-effective, sparsity usually must be structured. Modern accelerators (like NVIDIA’s) support a **fine-grained *structured* sparsity pattern: 2:4 sparsity**, meaning in each block of 4 weights, 2 are forced to zero ([Accelerating Inference with Sparsity Using the NVIDIA Ampere Architecture and NVIDIA TensorRT | NVIDIA Technical Blog](https://developer.nvidia.com/blog/accelerating-inference-with-sparsity-using-ampere-and-tensorrt/))  This pattern (50% zeros) is regular enough that hardware can skip the zeros efficiently. NVIDIA’s Ampere GPUs introduced sparse Tensor Cores that exploit this 2:4 pattern to achieve up to **2× throughput improvement** on matrix multiply operations ([Accelerating Inference with Sparsity Using the NVIDIA Ampere Architecture and NVIDIA TensorRT | NVIDIA Technical Blog](https://developer.nvidia.com/blog/accelerating-inference-with-sparsity-using-ampere-and-tensorrt/))  In practice, A100 GPUs realized ~30% overall performance/W gains on sparse-aware models vs dense ([Accelerating Inference with Sparsity Using the NVIDIA Ampere Architecture and NVIDIA TensorRT | NVIDIA Technical Blog](https://developer.nvidia.com/blog/accelerating-inference-with-sparsity-using-ampere-and-tensorrt/))  ([Accelerating Inference with Sparsity Using the NVIDIA Ampere Architecture and NVIDIA TensorRT | NVIDIA Technical Blog](https://developer.nvidia.com/blog/accelerating-inference-with-sparsity-using-ampere-and-tensorrt/))  The key is that unstructured random sparsity doesn’t translate to speed on general hardware (the irregular memory access and vector inefficiencies negate benefits) ([Accelerating Inference with Sparsity Using the NVIDIA Ampere Architecture and NVIDIA TensorRT | NVIDIA Technical Blog](https://developer.nvidia.com/blog/accelerating-inference-with-sparsity-using-ampere-and-tensorrt/))  But a pattern like 2-out-of-4 zeros has low metadata overhead and maps well to SIMD execution ([Accelerating Inference with Sparsity Using the NVIDIA Ampere Architecture and NVIDIA TensorRT | NVIDIA Technical Blog](https://developer.nvidia.com/blog/accelerating-inference-with-sparsity-using-ampere-and-tensorrt/))  Essentially, the multiplier array processes only the two nonzeros in each group, halving the operations needed ([Accelerating Inference with Sparsity Using the NVIDIA Ampere Architecture and NVIDIA TensorRT | NVIDIA Technical Blog](https://developer.nvidia.com/blog/accelerating-inference-with-sparsity-using-ampere-and-tensorrt/))
From an algorithmic standpoint, achieving 2:4 sparsity with minimal accuracy loss requires careful pruning. NVIDIA provided a recipe where models are trained with regularization that encourages that pattern, or one can fine-tune a pre-trained model and prune the smallest weights in each 4-group. Remarkably, many networks can be made 50% sparse in this structured way **with negligible accuracy drop**, especially if quantization is also in play (the two techniques can complement each other: quantization reduces precision, pruning removes entire weights, but both ultimately remove redundancy). Research on transformer models shows 2:4 sparsity can be applied to large language models and yield ~1.3× speedups at inference with minimal perplexity increase ([2:4 Sparse Llama: Smaller Models for Efficient GPU Inference](https://neuralmagic.com/blog/24-sparse-llama-smaller-models-for-efficient-gpu-inference/))  The speedup is a bit less than 2× ideal because of memory bottlenecks and non-pruned portions, but it’s still significant.
Because 2:4 sparsity was a hardware-supported feature, it effectively **renders unstructured sparsity inferior** on those platforms – there’s no point in 50% unstructured zeros that the hardware can’t accelerate, when you could enforce 50% structured zeros and get real speed gains. In the context of our survey, we include structured sparsity as an advanced technique because it directly targets hardware efficiency. One can consider it “quantization in the limit of zero,” i.e., 1-bit where that bit is zero or not. Indeed, some quantization methods deliberately incorporate sparsity (for example, if a weight is so small it quantizes to zero, that’s pruning it).
Beyond 2:4, other structured patterns have been explored: *N:M sparsity* in general (e.g. 4:8 etc.), block sparsity (entire neurons or channels pruned), and dynamic sparsity (activations like in BigSparse attention). The **proven one in real hardware is 2:4** on NVIDIA. It’s anticipated that future hardware may allow more flexibility (perhaps 1:4 or 4:8 patterns for different sparsity levels). Google’s TPUs and some accelerators can leverage unstructured sparsity in *software* by skipping based on zeros, but if not baked into hardware, the benefit is limited.
To summarize, **structured sparsity is a powerful complement to low-bit quantization**. A model that is both 4-bit and 50% sparse effectively has an arithmetic density of only 2 bits per weight (and in memory even less, since zeros need no storage if compressed). Many AI chips now support some form of this. We emphasize structured sparsity here because it *dominates unstructured pruning for performance*. Fine-grained unstructured pruning might reduce parameter count, but without hardware support it yields no speedup ([Accelerating Inference with Sparsity Using the NVIDIA Ampere Architecture and NVIDIA TensorRT | NVIDIA Technical Blog](https://developer.nvidia.com/blog/accelerating-inference-with-sparsity-using-ampere-and-tensorrt/))  Therefore, modern deployment focuses on patterns that hardware accelerates. By entirely omitting truly inferior methods (like random sparsity with no acceleration), we ensure the focus is on practical gains.
**Key takeaways on sparsity:**
- Enforce a regular sparsity pattern (like 2:4). It’s been shown to give ~1.5–2× speedups with ~0 loss in accuracy when done properly ([Accelerating Inference with Sparsity Using the NVIDIA Ampere Architecture and NVIDIA TensorRT | NVIDIA Technical Blog](https://developer.nvidia.com/blog/accelerating-inference-with-sparsity-using-ampere-and-tensorrt/))
- Avoid unstructured sparsity for on-chip speed – it doesn’t map well to vectorized hardware ([Accelerating Inference with Sparsity Using the NVIDIA Ampere Architecture and NVIDIA TensorRT | NVIDIA Technical Blog](https://developer.nvidia.com/blog/accelerating-inference-with-sparsity-using-ampere-and-tensorrt/))  Only use it for memory compression if needed, or use specialized engines that can handle it.

- Structured sparsity can be combined with quantization. In fact, if you quantize and half of the quantized values become 0, you’ve effectively implemented structured pruning as part of quantization. This synergy is often exploited in extreme compression setups.

- Currently, **NVIDIA GPUs have the clear advantage** of native sparse acceleration ([Accelerating Inference with Sparsity Using the NVIDIA Ampere Architecture and NVIDIA TensorRT | NVIDIA Technical Blog](https://developer.nvidia.com/blog/accelerating-inference-with-sparsity-using-ampere-and-tensorrt/))  Other hardware may follow (there are R&amp;D efforts for sparse TPU support, etc.), but if a platform doesn’t support it, then one might skip structured pruning on that platform to avoid wasted effort.

In the scope of maximizing inference efficiency, one should certainly consider using structured sparsity alongside advanced quantization – it is one of the few techniques (besides quantization) that yields multiplicative gains in efficiency.

## Emerging Analog Computing Paradigms
All the techniques discussed so far assume **digital computation** – numbers represented in bits, operations executed with Boolean logic. A fundamental shift on the horizon is the rise of **analog computing for AI**, which can potentially bypass some quantization limitations by performing computations in an analog domain (e.g., as electrical currents or voltages). While still in early stages, analog AI hardware has shown promise in achieving orders-of-magnitude efficiency gains. It’s worth including a brief section on this, separate from the main quantization strategies, because analog computing could be seen as an orthogonal approach to maximize efficiency.

In analog or **in-memory computing** accelerators, matrix multiplication is performed by physical processes (Kirchhoff’s laws in circuits, phase shifts in photonics, etc.) rather than digital multipliers. For instance, one popular approach uses **resistive memory crossbar arrays**: weights are stored as analog conductance values in non-volatile memory cells (like RRAM or phase-change memory), and input voltages are applied – by Ohm’s law and Kirchhoff’s current summation, the array computes a MAC operation in one step. The result is then converted back to digital. Because the multiplication and accumulation happen “for free” in the analog domain, the energy and time cost can be drastically lower than clocked digital operations. IBM Research recently demonstrated a **64-core analog AI chip** using phase-change memory that achieved accuracy on par with digital accelerators for vision tasks, while being significantly more energy-efficient ([An energy-efficient analog chip for AI inference - IBM Research](https://research.ibm.com/blog/analog-ai-chip-inference))  ([An energy-efficient analog chip for AI inference - IBM Research](https://research.ibm.com/blog/analog-ai-chip-inference))  It’s the first time an analog inference chip showed competitive accuracy at scale, a milestone indicating the approach’s viability.
From a quantization perspective, analog computing doesn’t fit the normal mold. The “weights” in analog memory might effectively be stored with, say, 6–8 bits of precision (limited by device physics and programming noise), and the accumulation might have even higher effective precision (since currents add with some inherent resolution). However, analog computation introduces other errors: device variations, thermal noise, nonlinear responses, etc. In a sense, deploying a model on an analog AI core is like quantizing it to an uncertain, non-uniform low-precision representation. Techniques have been developed to mitigate these errors, such as **hardware-aware training** where the model is trained while modeling the analog imperfections. Recent studies have managed to run models with 8-bit equivalent analog weights that actually *outperform 32-bit digital* in some cases by clever retraining ([Improving the accuracy of neural networks in analog computing-in ...](https://ieeexplore.ieee.org/document/9956368/)) (the analog noise acted like regularization) – though that’s a specific scenario. Generally, to use analog computing effectively, one must **calibrate the analog devices** to represent the trained weights as closely as possible and often retrain or fine-tune the model to compensate residual errors ([Improving the Accuracy of Analog-Based In-Memory Computing ...](https://arxiv.org/html/2401.09859v1))
**Analog vs Digital**: If analog computing matures, one could argue quantization as we know it becomes less of a concern – the limiting factor becomes how well the analog devices can store weights and for how long (drift), rather than how many bits we quantize to. In practice, any analog system will still have an effective bit precision (e.g., maybe 6-bit accuracy per MAC). The difference is that rather than systematically rounding weights to some format, you’re constrained by manufacturing variations and noise. Think of it as *stochastic quantization*: every weight is stored with slight error that may fluctuate over time. There are also hybrid analog-digital approaches, like doing the bulk of matrix multiply analog and then fine-tuning the result with a small digital correction.
At this stage, analog computing is **on the periphery of deployable tech** – companies like Mythic have built analog matrix-multiply chips (using analog flash cells) for low-power inference; HPE and others have prototypes with memristors; IBM’s research chip shows the high end potential. For our audience of AI silicon optimizers, it’s something to watch: analog could yield *fundamental leaps in efficiency* beyond what quantization and sparsity can do in digital (possibly 10-100× improvements in TOPS/W). But it comes with the challenge of maintaining accuracy without the deterministic precision of digital. The fundamental shifts here also include **photonic computing** (using light interference for computing – promising high speed and parallelism, but also requiring quantization of light intensity and facing noise issues), and **stochastic computing** (representing values as bit-stream probabilities – an old idea seeing minor revivals in some ML contexts).
Crucially, these analog approaches are not yet “proven” in mainstream use for large models. They remain speculative compared to the solidly proven methods of digital quantization we’ve focused on. Therefore, we mention them as an outlook: if one day you can offload your matrix multiplies to an analog crossbar that effectively performs an 8-bit operation at the cost of a 1-bit operation, that will change the game. It could reduce the need for extreme digital quantization, or conversely, you might *still* quantize (to reduce D/A conversion overhead) and push the analog to do even less work. In any case, analog computing is a complementary frontier aiming at the same goal: maximize inference efficiency.
To keep the core of this report practical, we won’t dive deeper into specific analog techniques. For now, the takeaway is: **analog AI hardware has demonstrated the ability to reach digital-comparable accuracy with higher energy efficiency** ([An energy-efficient analog chip for AI inference - IBM Research](https://research.ibm.com/blog/analog-ai-chip-inference))  but it operates under different constraints than digital quantization. As the technology matures, it may become another tool – or even a new paradigm – for deploying AI models efficiently.
## Industry Adoption and Outlook
With the array of quantization techniques and formats discussed, it’s illuminating to see how they are being adopted in real AI hardware. We will highlight only the **players with clear advantages or unique approaches** in quantization and exclude those without any edge (to maintain focus).

- **NVIDIA**: Nvidia has been at the forefront of low-precision support. Their A100 GPU introduced structured sparsity (2:4) and showed its benefits ([Accelerating Inference with Sparsity Using the NVIDIA Ampere Architecture and NVIDIA TensorRT | NVIDIA Technical Blog](https://developer.nvidia.com/blog/accelerating-inference-with-sparsity-using-ampere-and-tensorrt/))  ([Accelerating Inference with Sparsity Using the NVIDIA Ampere Architecture and NVIDIA TensorRT | NVIDIA Technical Blog](https://developer.nvidia.com/blog/accelerating-inference-with-sparsity-using-ampere-and-tensorrt/))  Their H100 GPU pioneered FP8 training and inference, proving that 8-bit is enough for even very large models. Now, Nvidia’s upcoming Blackwell generation is set to support **FP4 with block scaling (MXFP4)**, which will likely make 4-bit weight inference mainstream. Nvidia’s advantage is an end-to-end support: software (TensorRT, CUTLASS libraries) already supports these formats and will automatically apply them. For example, NVIDIA’s cuBLASLt supports FP4 (E2M1) with a per-16 scale (UE4M3) as a first-class data type ([cuBLAS](https://docs.nvidia.com/cuda/pdf/CUBLAS_Library.pdf))  This means quantization techniques like those in QuIP# can directly leverage hardware support: one could quantize a model’s weights to FP4 and run on Blackwell with actual 4-bit matrix operations. Nvidia has also shown willingness to incorporate research: e.g., adopting new formats from the OCP standard and possibly even looking into finer-grained sparsity or compression. Overall, Nvidia currently **dominates in deployable ultra-low precision** simply by virtue of having the hardware that supports it and the ecosystem to utilize it.
- **Google (TPU)**: Google’s TPUs have historically emphasized bfloat16 for training and int8 for inference. Google demonstrated int8 quantization at scale in services like Translate and search; they often use quantization-aware training or high-precision calibration to get int8 models without accuracy loss ([LLM-FP4: 4-Bit Floating-Point Quantized Transformers - ACL Anthology](https://aclanthology.org/2023.emnlp-main.39/))  While TPUs did not initially support FP8 when Nvidia did, Google is part of the OCP effort on new formats and has indicated interest in low-precision for future TPUs. TPU v4 has efficient int8, and it would not be surprising if TPU v5 supports FP8 or even block FP4 given industry trends. That said, as of now Google’s advantage is more on the algorithmic side (they published techniques like quantization-aware training methods, KLD calibration, etc., though those are now well-known). Hardware-wise, TPUs haven’t publicly claimed a unique quantization feature beyond what others have. We include Google mainly because their **system-level achievement** – running production translation models with 8-bit weights/acts – proved the viability of quantization in mission-critical applications, giving confidence to the whole field. For audiences optimizing AI silicon, Google’s practice underscores the importance of robust quantization schemes that can handle production edge cases.
- **Intel (Habana) and AMD**: Intel’s Habana Gaudi2 chips implemented FP8 support early (after Nvidia but in the same timeframe), showing that new entrants also see low precision as key. Intel is also actively involved in standardizing formats (the OpenVINO toolkit already supports MXFP4 quantization for CPUs) ([Microscaling (MX) Quantization — OpenVINO™  documentation](https://docs.openvino.ai/2024/openvino-workflow/model-optimization-guide/weight-compression/microscaling-quantization.html))  One interesting note from Intel’s open-source work: they observed that on CPU, FP4 with a float scale doesn’t give a speed boost over int8 (because CPUs lack native 4-bit instructions), but it can improve accuracy of compressed models ([Microscaling (MX) Quantization — OpenVINO™  documentation](https://docs.openvino.ai/2024/openvino-workflow/model-optimization-guide/weight-compression/microscaling-quantization.html))  This hints that in pure software environments, you might compress to FP4 for storage but decompress to run. In any case, Intel’s support of these formats in toolkits means they are preparing for hardware that can use them in the future. AMD, similarly, is a participant in the OCP microscaling initiative. The latest AMD GPUs (MI300) are expected to have competitive low-precision capabilities (FP8, int4, etc.), though details are sparse. Neither Intel nor AMD currently surpass Nvidia in quantization features, but their **real advantage** is ensuring the ecosystem isn’t proprietary – via open standards (OCP, IEEE) they enable broader adoption of low-precision techniques across platforms.

- **Graphcore**: Graphcore’s IPU is a bit different architecture (many independent cores with local memory). The IPU supports **arbitrary bit precision integers** in its compute units, which gives flexibility for research. Graphcore has showcased 4-bit weight quantization on models like GPT-J 6B, achieving about a 1.5× speedup and 4× memory reduction with no fine-tuning needed ([gpt-j/README.md at main · graphcore/gpt-j · GitHub](https://github.com/graphcore/gpt-j/blob/main/README.md))  They used a group quantization approach (similar to block scaling) for that demo. The fact that it sped up only 1.5× (not 2×) reveals a common theme: memory bandwidth often bottlenecks gains from quantization, something Graphcore’s design tries to mitigate with huge on-chip memory. Graphcore’s real advantage might be memory bandwidth and software control, which can better exploit sparsity or low precision by ensuring data is in the right place. They also use stochastic rounding in low precision, which can improve accuracy during training. While Graphcore is a smaller player, they are often early to experiment with techniques (they provided early software for FP8 and int4 experiments). In summary, Graphcore’s IPU demonstrates that **with sufficient architectural support (in memory and compute), 4-bit inference is feasible today** on non-Nvidia hardware. It reinforces that the advanced quantization techniques are not theoretical – they are being exercised on multiple platforms.
- **Qualcomm (Edge devices)**: In the mobile/edge domain, Qualcomm has been a leader in quantization. Their Hexagon DSP for AI in Snapdragon chips added **INT4 support** in recent generations, yielding up to 60% performance improvement over int8 on the same hardware and 4× higher throughput than their previous generation that lacked int4 ([Hexagon Adds INT4 Support | TechInsights](https://techinsights.com/blog/hexagon-adds-int4-support))  This is a big deal for on-device AI, where power is limited. Qualcomm has actually argued that for edge use, **integer quantization is preferable to floating-point** because of simpler hardware and power efficiency ([Floating-point arithmetic for AI inference — hit or miss? | Qualcomm](https://qualcomm.com/news/onq/2023/04/floating-point-arithmetic-for-ai-inference-hit-or-miss))  A Qualcomm whitepaper compared an FP8 approach to an INT8 approach on mobile models and found int8 equally accurate and more efficient for that context ([Floating-point arithmetic for AI inference — hit or miss? | Qualcomm](https://qualcomm.com/news/onq/2023/04/floating-point-arithmetic-for-ai-inference-hit-or-miss))  This suggests that while FP8/FP4 are great for datacenter GPUs (which handle a wide range of values and can afford slightly more complex logic), in a highly power-constrained scenario a fixed-point format with carefully calibrated scaling might be optimal. Qualcomm’s advantage is their end-to-end view: they tailor quantization (including custom schemes like mixed precision or per-channel scaling) to their hardware, and they’ve enabled developers with tools to quantize models to int8 or int4 for the Hexagon NPU. Practically, this means **4-bit inference is also happening on smartphones** (for suitable models), making advanced quantization truly ubiquitous from cloud to edge.

- **Analog/Neuromorphic startups**: A few companies are pushing analog computing (as discussed) – e.g., Mythic had an analog matrix accelerator storing 8-bit weights in analog flash cells. While Mythic’s early products targeted int8 equivalence, one could envision analog chips that naturally operate at some “effective bit precision” but very efficiently. Another domain is **neuromorphic chips** (like IBM TrueNorth, Intel Loihi) which use spiking neurons and effectively low-bit communication (spike events). Those are more research-oriented and haven’t proven superior on large-scale AI tasks yet. We mention these players only insofar as they might leapfrog in specific niches: for instance, an analog compute-in-memory chip could do 4-bit dot products massively in parallel with extremely low power, which would be an advantage if it can hit required accuracy. These are still emerging; none outright dominate traditional digital hardware across the board. But in a few years, if analog designs achieve general reliability, they may become the go-to for ultra-efficient inference, relegating some of our digital quantization tricks to secondary importance.

In conclusion, the industry trend is clear: **nearly every major AI hardware player is investing in low-precision quantization support.** Nvidia leads in deployed capability (FP8, FP4, sparsity), while others are fast-following through open standards and unique twists (Graphcore’s flexibility, Qualcomm’s edge focus). Companies not keeping up in this space will be left behind – we intentionally haven’t detailed any that are “outperformed in all areas.” For example, if a hypothetical company’s accelerator only supports FP16 and nothing else, it offers no advantage in quantization and wouldn’t be relevant to this discussion. Fortunately, most have recognized the necessity: even conservative CPU vendors are enabling 8-bit and exploring 4-bit, and GPU/TPU designers have made quantization a first-class citizen in architecture design.
**Outlook:** With techniques like those surveyed (Hadamard PTQ, lattice and trellis quantization, mixed precision, etc.), it’s now possible to take large neural networks and compress them by 4–8× or more *with minimal loss*, and deploy them on specialized hardware that runs them several times faster at lower power. This directly translates to cost savings and new capabilities (e.g., running an LLM on a smaller edge device). The **focus on hardware efficiency** means that these quantization methods are chosen not just for compression, but for how well they map to real hardware instructions and parallelism. The field has largely moved past toy schemes and now emphasizes *practical, deployable strategies*. Going forward, we expect continued convergence of algorithmic innovation and hardware support. For instance, future hardware might include even finer-grained scaling (block size 8 or per-channel FP4) if research shows it’s needed, or support for on-the-fly decompression of codebook-based weights if vector quantization remains superior. At the same time, algorithm developers will consider hardware constraints even more – perhaps limiting themselves to quantization formats that avoid expensive operations, or developing one-shot quantization algorithms that integrate into the compilation toolchain of hardware.
In summary, the **state-of-the-art quantization toolbox** is now very rich: from advanced PTQ methods that can quantize to 2–4 bits, to specialized numeric formats like FP4, to complementary sparsity and even analog compute. Using these tools, AI practitioners aiming to optimize silicon performance have an unprecedented ability to trade off negligible accuracy for massive efficiency gains. The methods highlighted here represent the current pinnacle of that effort – each chosen for being **best-in-class or uniquely promising**. By deploying these proven techniques, one can ensure maximal inference efficiency on modern AI accelerators, staying ahead of the curve as models grow and demands increase.
**References:** (All citations in text)

([[2402.04396] QuIP#: Even Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks](https://arxiv.org/abs/2402.04396))  ([[2402.04396] QuIP#: Even Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks](https://arxiv.org/abs/2402.04396)) Tseng et al., *“QuIP#: Even Better LLM Quantization with Hadamard Incoherence and Lattice Codebooks,”* ICML 2024 – introduced QuIP#, weight-only PTQ ≤4 bits, uses Hadamard transforms and E8 lattice codebooks, outperforms prior PTQ.

([Even Better, Even Faster Quantized LLMs with QTIP](https://together.ai/blog/even-better-even-faster-quantized-llms-with-qtip))  ([Even Better, Even Faster Quantized LLMs with QTIP](https://together.ai/blog/even-better-even-faster-quantized-llms-with-qtip)) Together AI Blog – *“Even Better, Even Faster Quantized LLMs with QTIP,”* Oct 2024 – describes QTIP, uses Hadamard + Trellis Coded Quantization, lower distortion than VQ (QuIP#, AQLM) on Gaussian weights.

([GitHub - pprp/Awesome-LLM-Quantization: Awesome list for LLM quantization](https://github.com/pprp/Awesome-LLM-Quantization)) Chen et al., *“AQLM: Extreme Compression of LLMs via Additive Quantization,”* ACL 2024 – demonstrates multi-codebook additive quantization, achieving Pareto-optimal results under 3 bits, outperforming existing methods at 2 bits.

([Accelerating Inference with Sparsity Using the NVIDIA Ampere Architecture and NVIDIA TensorRT | NVIDIA Technical Blog](https://developer.nvidia.com/blog/accelerating-inference-with-sparsity-using-ampere-and-tensorrt/))  ([Accelerating Inference with Sparsity Using the NVIDIA Ampere Architecture and NVIDIA TensorRT | NVIDIA Technical Blog](https://developer.nvidia.com/blog/accelerating-inference-with-sparsity-using-ampere-and-tensorrt/)) NVIDIA Technical Blog – *“Accelerating Inference with Sparsity on Ampere GPUs,”* July 2021 – explains 2:4 structured sparsity support, 50% of weights pruned in pattern yields ~2× speed, 30%+ perf/W gains.

([cuBLAS](https://docs.nvidia.com/cuda/pdf/CUBLAS_Library.pdf)) NVIDIA cuBLASLt Documentation, 2023 – details of FP4 (E2M1) with UE4M3 block scaling: one 8-bit scale per 16-element block in memory.
([CUDA - Wikipedia](https://en.wikipedia.org/wiki/CUDA)) Wikipedia – CUDA data formats – defines UE4M3 (7-bit unsigned FP8 used as scale for E2M1 FP4) and FP8 E4M3, E5M2 formats (Nvidia, others).

([LLM-FP4: 4-Bit Floating-Point Quantized Transformers - ACL Anthology](https://aclanthology.org/2023.emnlp-main.39/))  ([LLM-FP4: 4-Bit Floating-Point Quantized Transformers - ACL Anthology](https://aclanthology.org/2023.emnlp-main.39/)) Liu et al., *“LLM-FP4: 4-Bit Floating-Point Quantized Transformers,”* EMNLP 2023 – shows FP4 (with carefully chosen exponent bits and per-channel scaling) outperforms integer PTQ below 8-bit, enabling LLaMA-13B W4A4 with minimal loss (5.8 drop, 12.7 better than prior SOTA).
([GitHub - pprp/Awesome-LLM-Quantization: Awesome list for LLM quantization](https://github.com/pprp/Awesome-LLM-Quantization)) Chee et al., *“QuIP: 2-bit Quantization of LLMs with Guarantees,”* ICLR 2024 (spotlight) – introduced the original QuIP, made 2-bit viable via adaptive rounding and orthogonal transforms (random rotations) to decorrelate weights.
([gpt-j/README.md at main · graphcore/gpt-j · GitHub](https://github.com/graphcore/gpt-j/blob/main/README.md)) Graphcore GPT-J IPU Notebook – *“Faster Text Generation with GPT-J using 4-bit Weight Quantization on IPUs,”* 2022 – demonstrates group-wise INT4 on IPU, 4× memory reduction and ~1.5× speedup, no fine-tuning.
([Hexagon Adds INT4 Support | TechInsights](https://techinsights.com/blog/hexagon-adds-int4-support)) TechInsights – *“Hexagon Adds INT4 Support,”* Nov 2022 – reports Qualcomm Snapdragon Gen2 Hexagon DSP added 4-bit weight support, increasing AI performance ~60% over previous gen and targeting 4× improvement in peak throughput (int4 vs int8).
([An energy-efficient analog chip for AI inference - IBM Research](https://research.ibm.com/blog/analog-ai-chip-inference)) IBM Research Blog – *“An energy-efficient analog AI inference chip,”* Aug 2023 – announces a 64-core analog in-memory compute chip (PCM based) that achieved accuracy on vision tasks comparable to digital systems, with far lower energy, marking a first in analog AI hardware.
